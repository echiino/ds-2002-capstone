{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bd363f-f36b-47ba-b633-9da933a4cb77",
   "metadata": {},
   "source": [
    "### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62910289-e06f-4707-a070-04810b6b0f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\spark-3.5.4-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\" # for some reason if i don't do this then it won't create a new spark session ...\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f0177-3359-4311-a76a-05cb2f7adfeb",
   "metadata": {},
   "source": [
    "### 2.0. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b45c135-571a-434a-8148-235508c78931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"chinook\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"your_mysql_username\",\n",
    "        \"password\" : \"your_mysql_password\",\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"cluster_location\" : \"atlas\", # or \"local\" if running MongoDB locally\n",
    "    \"user_name\" : \"your_mongodb_username\",\n",
    "    \"password\" : \"your_mongodb_password\",\n",
    "    \"cluster_name\" : \"your_cluster_name\",\n",
    "    \"cluster_subnet\" : \"your_cluster_subnet\",\n",
    "    \"db_name\" : \"chinook\",\n",
    "    \"collection\" : \"\",\n",
    "    \"null_column_threshold\" : 0.5\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'chinook')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "invoices_stream_dir = os.path.join(stream_dir, 'invoices')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"chinook_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "invoice_sales_output_bronze = os.path.join(database_dir, 'fact_invoice_sales', 'bronze')\n",
    "invoice_sales_output_silver = os.path.join(database_dir, 'fact_invoice_sales', 'silver')\n",
    "invoice_sales_output_gold = os.path.join(database_dir, 'fact_invoice_sales', 'gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d71ec9-6229-483f-99f3-c7a958cd7bcc",
   "metadata": {},
   "source": [
    "### 3.0 Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14359fb0-8fa3-4e22-88a0-74fd83f022a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    \n",
    "    return df_dropped\n",
    "    \n",
    "    \n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Chinook Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "    \n",
    "    \n",
    "# TODO: Rewrite this to leverage PySpark?\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r', encoding='utf-8') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    \n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671913ea-c599-4b46-b91d-7c29e77f453b",
   "metadata": {},
   "source": [
    "### 4.0. Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "39fe6303-fcb3-4879-8c44-bcda6927eb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory 'C:\\\\Users\\\\emili\\\\ds2002\\\\ds-2002-capstone\\\\spark-warehouse\\\\chinook_dlh.db' has been removed successfully.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc99c9e-6894-4695-a1c6-b1956d8fbc12",
   "metadata": {},
   "source": [
    "### 5.0. Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e271f4f-dedc-4aba-97fc-b710a0bf83a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kubernetes.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Chinook Data Lakehouse (Medallion Architecture)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c35b2e2030>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48ba69e-0bc6-43ab-b676-2aa168dc80c0",
   "metadata": {},
   "source": [
    "### 6.0. Create a New Metadata Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93af02c6-482f-4f6a-9c21-d424b443f180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Capstone Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Capstone');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f407338-7c03-488c-8edc-c26d0a514eaf",
   "metadata": {},
   "source": [
    "## Section II: Populate Dimensions by Ingesting \"Cold-path\" Reference Data \n",
    "### 1.0. Fetch Data from the File System\n",
    "#### 1.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026d95e5-0499-4734-b3e1-20b26c95a64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chinook_albums.csv</td>\n",
       "      <td>11366</td>\n",
       "      <td>2025-12-15 06:36:48.386648178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chinook_artists.csv</td>\n",
       "      <td>7434</td>\n",
       "      <td>2025-12-15 06:36:59.900135040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chinook_customers.json</td>\n",
       "      <td>20081</td>\n",
       "      <td>2025-12-15 06:37:49.282068253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chinook_genres.csv</td>\n",
       "      <td>346</td>\n",
       "      <td>2025-12-15 06:37:15.528573274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinook_mediatypes.csv</td>\n",
       "      <td>146</td>\n",
       "      <td>2025-12-15 06:37:40.556010723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chinook_tracks.csv</td>\n",
       "      <td>70308</td>\n",
       "      <td>2025-12-15 06:36:31.412338734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name   size             modification_time\n",
       "0      chinook_albums.csv  11366 2025-12-15 06:36:48.386648178\n",
       "1     chinook_artists.csv   7434 2025-12-15 06:36:59.900135040\n",
       "2  chinook_customers.json  20081 2025-12-15 06:37:49.282068253\n",
       "3      chinook_genres.csv    346 2025-12-15 06:37:15.528573274\n",
       "4  chinook_mediatypes.csv    146 2025-12-15 06:37:40.556010723\n",
       "5      chinook_tracks.csv  70308 2025-12-15 06:36:31.412338734"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(batch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d367807-26fd-4f97-8778-0e0c8220ad51",
   "metadata": {},
   "source": [
    "#### 1.2. Populate the <span style=\"color:darkred\">Track Dimension</span>\n",
    "##### 1.2.1. Use PySpark to Read data from CSV files (track, album, artist, genre, mediatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881262a6-52e8-42a8-95ce-5107841bb20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\ds2002\\ds-2002-capstone\\lab_data\\chinook\\batch\\chinook_tracks.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrackId</th>\n",
       "      <th>Name</th>\n",
       "      <th>AlbumId</th>\n",
       "      <th>MediaTypeId</th>\n",
       "      <th>GenreId</th>\n",
       "      <th>Composer</th>\n",
       "      <th>Milliseconds</th>\n",
       "      <th>Bytes</th>\n",
       "      <th>UnitPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>For Those About To Rock (We Salute You)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Angus Young, Malcolm Young, Brian Johnson</td>\n",
       "      <td>343719</td>\n",
       "      <td>11170334</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Balls to the Wall</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>U. Dirkschneider, W. Hoffmann, H. Frank, P. Ba...</td>\n",
       "      <td>342562</td>\n",
       "      <td>5510424</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TrackId                                     Name  AlbumId  MediaTypeId  \\\n",
       "0        1  For Those About To Rock (We Salute You)        1            1   \n",
       "1        2                        Balls to the Wall        2            2   \n",
       "\n",
       "   GenreId                                           Composer  Milliseconds  \\\n",
       "0        1          Angus Young, Malcolm Young, Brian Johnson        343719   \n",
       "1        1  U. Dirkschneider, W. Hoffmann, H. Frank, P. Ba...        342562   \n",
       "\n",
       "      Bytes  UnitPrice  \n",
       "0  11170334       0.99  \n",
       "1   5510424       0.99  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_csv = os.path.join(batch_dir, 'chinook_tracks.csv')\n",
    "print(track_csv)\n",
    "\n",
    "df_tracks = spark.read.format('csv').options(header='true', inferSchema='true').load(track_csv)\n",
    "df_tracks.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed8a9593-d49c-44fd-b4fc-07248050184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\ds2002\\ds-2002-capstone\\lab_data\\chinook\\batch\\chinook_albums.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AlbumId</th>\n",
       "      <th>Title</th>\n",
       "      <th>ArtistId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>For Those About To Rock We Salute You</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Balls to the Wall</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AlbumId                                  Title  ArtistId\n",
       "0        1  For Those About To Rock We Salute You         1\n",
       "1        2                      Balls to the Wall         2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "album_csv = os.path.join(batch_dir, 'chinook_albums.csv')\n",
    "print(album_csv)\n",
    "\n",
    "df_albums = spark.read.format('csv').options(header='true', inferSchema='true').load(album_csv)\n",
    "df_albums.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b4fea1-c93e-48f7-ba66-cd75102ec4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\ds2002\\ds-2002-capstone\\lab_data\\chinook\\batch\\chinook_artists.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArtistId</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AC/DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Accept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArtistId    Name\n",
       "0         1   AC/DC\n",
       "1         2  Accept"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_csv = os.path.join(batch_dir, 'chinook_artists.csv')\n",
    "print(artist_csv)\n",
    "\n",
    "df_artists = spark.read.format('csv').options(header='true', inferSchema='true').load(artist_csv)\n",
    "df_artists.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576bd031-fe5a-42a2-bf0b-4a3f8c8f2186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\ds2002\\ds-2002-capstone\\lab_data\\chinook\\batch\\chinook_genres.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GenreId</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jazz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GenreId  Name\n",
       "0        1  Rock\n",
       "1        2  Jazz"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_csv = os.path.join(batch_dir, 'chinook_genres.csv')\n",
    "print(genre_csv)\n",
    "\n",
    "df_genres = spark.read.format('csv').options(header='true', inferSchema='true').load(genre_csv)\n",
    "df_genres.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "765bba39-7b9f-4c18-bbc6-191381a1a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\ds2002\\ds-2002-capstone\\lab_data\\chinook\\batch\\chinook_mediatypes.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MediaTypeId</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MPEG audio file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Protected AAC audio file</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MediaTypeId                      Name\n",
       "0            1           MPEG audio file\n",
       "1            2  Protected AAC audio file"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mediatype_csv = os.path.join(batch_dir, 'chinook_mediatypes.csv')\n",
    "print(mediatype_csv)\n",
    "\n",
    "df_mediatypes = spark.read.format('csv').options(header='true', inferSchema='true').load(mediatype_csv)\n",
    "df_mediatypes.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2e1aa-e943-4f3f-9c30-9db275469f95",
   "metadata": {},
   "source": [
    "##### 1.2.2. Make Necessary Transformations to the New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95e7ddc2-55ac-41d2-97b1-75b95215b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename columns (fix casing)\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_tracks = (\n",
    "    df_tracks\n",
    "    .withColumnRenamed(\"TrackId\", \"track_id\")\n",
    "    .withColumnRenamed(\"Name\", \"track_name\")\n",
    "    .withColumnRenamed(\"AlbumId\", \"album_id\")\n",
    "    .withColumnRenamed(\"GenreId\", \"genre_id\")\n",
    "    .withColumnRenamed(\"MediaTypeId\", \"media_type_id\")\n",
    "    .withColumnRenamed(\"UnitPrice\", \"unit_price\")\n",
    ")\n",
    "\n",
    "df_albums = (\n",
    "    df_albums\n",
    "    .withColumnRenamed(\"Title\", \"album_title\")\n",
    "    .withColumnRenamed(\"AlbumId\", \"album_id\")\n",
    "    .withColumnRenamed(\"ArtistId\", \"artist_id\")\n",
    ")\n",
    "\n",
    "df_artists = (\n",
    "    df_artists\n",
    "    .withColumnRenamed(\"Name\", \"artist_name\")\n",
    "    .withColumnRenamed(\"ArtistId\", \"artist_id\")\n",
    ")\n",
    "\n",
    "df_genres = (\n",
    "    df_genres\n",
    "    .withColumnRenamed(\"Name\", \"genre_name\")\n",
    "    .withColumnRenamed(\"GenreId\", \"genre_id\")\n",
    ")\n",
    "\n",
    "df_mediatypes = (\n",
    "    df_mediatypes\n",
    "    .withColumnRenamed(\"Name\", \"media_type\")\n",
    "    .withColumnRenamed(\"MediaTypeId\", \"media_type_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ae4e0f-fcda-4f3d-abcc-0240817efd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Merge Tables (join tracks w/ albuns, artists, genres, mediatypes)\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_track = ( \n",
    "    df_tracks .join(df_albums, \"album_id\", \"left\") \n",
    "    .join(df_artists, \"artist_id\", \"left\") \n",
    "    .join(df_genres, \"genre_id\", \"left\")\n",
    "    .join(df_mediatypes, \"media_type_id\", \"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c49728da-6e30-40cf-8704-22b37f7ce721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_key</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>album_title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>genre_name</th>\n",
       "      <th>media_type</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>For Those About To Rock (We Salute You)</td>\n",
       "      <td>For Those About To Rock We Salute You</td>\n",
       "      <td>AC/DC</td>\n",
       "      <td>Rock</td>\n",
       "      <td>MPEG audio file</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Balls to the Wall</td>\n",
       "      <td>Balls to the Wall</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Protected AAC audio file</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_key  track_id                               track_name  \\\n",
       "0          1         1  For Those About To Rock (We Salute You)   \n",
       "1          2         2                        Balls to the Wall   \n",
       "\n",
       "                             album_title artist_name genre_name  \\\n",
       "0  For Those About To Rock We Salute You       AC/DC       Rock   \n",
       "1                      Balls to the Wall      Accept       Rock   \n",
       "\n",
       "                 media_type  unit_price  \n",
       "0           MPEG audio file        0.99  \n",
       "1  Protected AAC audio file        0.99  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_track.createOrReplaceTempView(\"tracks\")\n",
    "\n",
    "sql_dim_track = \"\"\"\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (ORDER BY track_id) AS track_key\n",
    "    FROM tracks\n",
    "\"\"\"\n",
    "\n",
    "df_dim_track = spark.sql(sql_dim_track)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Rename/Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "ordered_columns = [\n",
    "    \"track_key\",  \"track_id\",  \"track_name\", \"album_title\", \"artist_name\", \"genre_name\", \"media_type\", \"unit_price\"]\n",
    "\n",
    "df_dim_track = df_dim_track[ordered_columns]\n",
    "df_dim_track.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376fe4c-5fa6-4168-9744-449993834219",
   "metadata": {},
   "source": [
    "##### 1.2.3. Save as the <span style=\"color:darkred\">dim_track</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3dc70f6-7c9c-4d53-9d83-51c68f322819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_track.write.saveAsTable(f\"{dest_database}.dim_track\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bf172-2b6f-4896-945e-97079f589285",
   "metadata": {},
   "source": [
    "##### 1.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdde2d02-9d78-4a75-b426-c2e2c5708945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|           track_key|                 int|   NULL|\n",
      "|            track_id|                 int|   NULL|\n",
      "|          track_name|              string|   NULL|\n",
      "|         album_title|              string|   NULL|\n",
      "|         artist_name|              string|   NULL|\n",
      "|          genre_name|              string|   NULL|\n",
      "|          media_type|              string|   NULL|\n",
      "|          unit_price|              double|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|         chinook_dlh|       |\n",
      "|               Table|           dim_track|       |\n",
      "|        Created Time|Mon Dec 15 22:31:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/C:/Users/em...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_key</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>album_title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>genre_name</th>\n",
       "      <th>media_type</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>For Those About To Rock (We Salute You)</td>\n",
       "      <td>For Those About To Rock We Salute You</td>\n",
       "      <td>AC/DC</td>\n",
       "      <td>Rock</td>\n",
       "      <td>MPEG audio file</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Balls to the Wall</td>\n",
       "      <td>Balls to the Wall</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Protected AAC audio file</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_key  track_id                               track_name  \\\n",
       "0          1         1  For Those About To Rock (We Salute You)   \n",
       "1          2         2                        Balls to the Wall   \n",
       "\n",
       "                             album_title artist_name genre_name  \\\n",
       "0  For Those About To Rock We Salute You       AC/DC       Rock   \n",
       "1                      Balls to the Wall      Accept       Rock   \n",
       "\n",
       "                 media_type  unit_price  \n",
       "0           MPEG audio file        0.99  \n",
       "1  Protected AAC audio file        0.99  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_track;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_track LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf0a95-dbd9-4b93-82fe-78e1d257f50b",
   "metadata": {},
   "source": [
    "### 2.0. Fetch Reference Data from a MongoDB Atlas Database\n",
    "#### 2.1. Create a New MongoDB Database, and Load JSON File into a New MongoDB Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "691b7979-651b-42e5-a1df-0cc29463e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "json_files = {\"customers\" : \"chinook_customers.json\"}\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], batch_dir, json_files) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999d558-8d7b-4007-a53a-a4cdbe7b63b9",
   "metadata": {},
   "source": [
    "#### 2.2. Populate the <span style=\"color:darkred\">Customers Dimension</span>\n",
    "##### 2.2.1. Fetch Data from the New MongoDB <span style=\"color:darkred\">Customers</span> Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a86090f-3e07-47de-9a05-57cb70ae0984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Email</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>Phone</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>State</th>\n",
       "      <th>SupportRepId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Av. Brigadeiro Faria Lima, 2170</td>\n",
       "      <td>São José dos Campos</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>1</td>\n",
       "      <td>luisg@embraer.com.br</td>\n",
       "      <td>Luís</td>\n",
       "      <td>Gonçalves</td>\n",
       "      <td>+55 (12) 3923-5555</td>\n",
       "      <td>12227-000</td>\n",
       "      <td>SP</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Theodor-Heuss-Straße 34</td>\n",
       "      <td>Stuttgart</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2</td>\n",
       "      <td>leonekohler@surfeu.de</td>\n",
       "      <td>Leonie</td>\n",
       "      <td>Köhler</td>\n",
       "      <td>+49 0711 2842222</td>\n",
       "      <td>70174</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Address                 City  Country  CustomerId  \\\n",
       "0  Av. Brigadeiro Faria Lima, 2170  São José dos Campos   Brazil           1   \n",
       "1          Theodor-Heuss-Straße 34            Stuttgart  Germany           2   \n",
       "\n",
       "                   Email FirstName   LastName               Phone PostalCode  \\\n",
       "0   luisg@embraer.com.br      Luís  Gonçalves  +55 (12) 3923-5555  12227-000   \n",
       "1  leonekohler@surfeu.de    Leonie     Köhler    +49 0711 2842222      70174   \n",
       "\n",
       "  State  SupportRepId  \n",
       "0    SP             3  \n",
       "1  None             5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongodb_args[\"collection\"] = \"customers\"\n",
    "\n",
    "df_dim_customers = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154e482-2e6a-4964-9068-35779531c229",
   "metadata": {},
   "source": [
    "##### 2.2.2. Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c60a9b1-c9b4-47b3-97c2-6fc953fef89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>support_rep_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Luís</td>\n",
       "      <td>Gonçalves</td>\n",
       "      <td>+55 (12) 3923-5555</td>\n",
       "      <td>luisg@embraer.com.br</td>\n",
       "      <td>Av. Brigadeiro Faria Lima, 2170</td>\n",
       "      <td>São José dos Campos</td>\n",
       "      <td>SP</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>12227-000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Leonie</td>\n",
       "      <td>Köhler</td>\n",
       "      <td>+49 0711 2842222</td>\n",
       "      <td>leonekohler@surfeu.de</td>\n",
       "      <td>Theodor-Heuss-Straße 34</td>\n",
       "      <td>Stuttgart</td>\n",
       "      <td>None</td>\n",
       "      <td>Germany</td>\n",
       "      <td>70174</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id first_name  last_name               phone  \\\n",
       "0             1            1       Luís  Gonçalves  +55 (12) 3923-5555   \n",
       "1             2            2     Leonie     Köhler    +49 0711 2842222   \n",
       "\n",
       "                   email                          address  \\\n",
       "0   luisg@embraer.com.br  Av. Brigadeiro Faria Lima, 2170   \n",
       "1  leonekohler@surfeu.de          Theodor-Heuss-Straße 34   \n",
       "\n",
       "                  city state  country postal_code  support_rep_id  \n",
       "0  São José dos Campos    SP   Brazil   12227-000               3  \n",
       "1            Stuttgart  None  Germany       70174               5  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'customer_id' + fix other column names ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "df_dim_customers = df_dim_customers.withColumnRenamed(\"id\", \"customer_id\")\n",
    "\n",
    "df_dim_customers = (\n",
    "    df_dim_customers\n",
    "    .withColumnRenamed(\"CustomerId\", \"customer_id\")\n",
    "    .withColumnRenamed(\"Address\", \"address\")\n",
    "    .withColumnRenamed(\"City\", \"city\")\n",
    "    .withColumnRenamed(\"Country\", \"country\")\n",
    "    .withColumnRenamed(\"Email\", \"email\")\n",
    "    .withColumnRenamed(\"FirstName\", \"first_name\")\n",
    "    .withColumnRenamed(\"LastName\", \"last_name\")\n",
    "    .withColumnRenamed(\"Phone\", \"phone\")\n",
    "    .withColumnRenamed(\"PostalCode\", \"postal_code\")\n",
    "    .withColumnRenamed(\"State\", \"state\")\n",
    "    .withColumnRenamed(\"SupportRepId\", \"support_rep_id\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using the SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "df_dim_customers.createOrReplaceTempView(\"customers\")\n",
    "sql_customers = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY customer_id) AS customer_key\n",
    "    FROM customers;\n",
    "\"\"\"\n",
    "df_dim_customers = spark.sql(sql_customers)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "ordered_columns = ['customer_key', 'customer_id', 'first_name', 'last_name'\n",
    "                   , 'phone', 'email' , 'address', 'city', 'state', 'country'\n",
    "                   , 'postal_code', 'support_rep_id']\n",
    "\n",
    "df_dim_customers = df_dim_customers[ordered_columns]\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4d9ae-20de-44ca-9602-41639e16caf3",
   "metadata": {},
   "source": [
    "##### 2.2.3. Save as the <span style=\"color:darkred\">dim_customers</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1105509c-d7a8-4a2d-9f1a-d56a097f326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a488dba-1927-40a6-bdcf-9960311799d4",
   "metadata": {},
   "source": [
    "##### 2.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f9ce73f-9978-4d15-a58a-64970546a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|        customer_key|                 int|   NULL|\n",
      "|         customer_id|                 int|   NULL|\n",
      "|          first_name|              string|   NULL|\n",
      "|           last_name|              string|   NULL|\n",
      "|               phone|              string|   NULL|\n",
      "|               email|              string|   NULL|\n",
      "|             address|              string|   NULL|\n",
      "|                city|              string|   NULL|\n",
      "|               state|              string|   NULL|\n",
      "|             country|              string|   NULL|\n",
      "|         postal_code|              string|   NULL|\n",
      "|      support_rep_id|                 int|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|         chinook_dlh|       |\n",
      "|               Table|       dim_customers|       |\n",
      "|        Created Time|Mon Dec 15 22:32:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.4|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>support_rep_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Luís</td>\n",
       "      <td>Gonçalves</td>\n",
       "      <td>+55 (12) 3923-5555</td>\n",
       "      <td>luisg@embraer.com.br</td>\n",
       "      <td>Av. Brigadeiro Faria Lima, 2170</td>\n",
       "      <td>São José dos Campos</td>\n",
       "      <td>SP</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>12227-000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Leonie</td>\n",
       "      <td>Köhler</td>\n",
       "      <td>+49 0711 2842222</td>\n",
       "      <td>leonekohler@surfeu.de</td>\n",
       "      <td>Theodor-Heuss-Straße 34</td>\n",
       "      <td>Stuttgart</td>\n",
       "      <td>None</td>\n",
       "      <td>Germany</td>\n",
       "      <td>70174</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id first_name  last_name               phone  \\\n",
       "0             1            1       Luís  Gonçalves  +55 (12) 3923-5555   \n",
       "1             2            2     Leonie     Köhler    +49 0711 2842222   \n",
       "\n",
       "                   email                          address  \\\n",
       "0   luisg@embraer.com.br  Av. Brigadeiro Faria Lima, 2170   \n",
       "1  leonekohler@surfeu.de          Theodor-Heuss-Straße 34   \n",
       "\n",
       "                  city state  country postal_code  support_rep_id  \n",
       "0  São José dos Campos    SP   Brazil   12227-000               3  \n",
       "1            Stuttgart  None  Germany       70174               5  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_customers;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_customers LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650fa47-2a0e-4f2c-8013-22fa8710ee37",
   "metadata": {},
   "source": [
    "### 3.0. Fetch Reference Data from a MySQL Database\n",
    "#### 3.1. Populate the <span style=\"color:darkred\">Date Dimension</span>\n",
    "##### 3.1.1 Fetch data from the <span style=\"color:darkred\">dim_date</span> table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06071cb6-a5fc-419e-bb8d-a6c89249b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = f\"SELECT * FROM {mysql_args['db_name']}.dim_date\"\n",
    "df_dim_date = get_mysql_dataframe(spark, sql_dim_date, **mysql_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42cec5-22af-47b5-8624-a48c32b0c083",
   "metadata": {},
   "source": [
    "##### 3.1.2. Save as the <span style=\"color:darkred\">dim_date</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "642c5da3-21f8-4298-ae9a-374f39b57e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fd043-5631-45fe-9a71-b2ccdb181a5a",
   "metadata": {},
   "source": [
    "##### 3.1.3. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87ffef9e-9e5c-4538-9359-fde2f14d0de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|            date_key|      int|   NULL|\n",
      "|           full_date|     date|   NULL|\n",
      "|           date_name| char(11)|   NULL|\n",
      "|        date_name_us| char(11)|   NULL|\n",
      "|        date_name_eu| char(11)|   NULL|\n",
      "|         day_of_week|  tinyint|   NULL|\n",
      "|    day_name_of_week| char(10)|   NULL|\n",
      "|        day_of_month|  tinyint|   NULL|\n",
      "|         day_of_year|      int|   NULL|\n",
      "|     weekday_weekend| char(10)|   NULL|\n",
      "|        week_of_year|  tinyint|   NULL|\n",
      "|          month_name| char(10)|   NULL|\n",
      "|       month_of_year|  tinyint|   NULL|\n",
      "|is_last_day_of_month|  char(1)|   NULL|\n",
      "|    calendar_quarter|  tinyint|   NULL|\n",
      "|       calendar_year|      int|   NULL|\n",
      "| calendar_year_month| char(10)|   NULL|\n",
      "|   calendar_year_qtr| char(10)|   NULL|\n",
      "|fiscal_month_of_year|  tinyint|   NULL|\n",
      "|      fiscal_quarter|  tinyint|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date_name</th>\n",
       "      <th>date_name_us</th>\n",
       "      <th>date_name_eu</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>is_last_day_of_month</th>\n",
       "      <th>calendar_quarter</th>\n",
       "      <th>calendar_year</th>\n",
       "      <th>calendar_year_month</th>\n",
       "      <th>calendar_year_qtr</th>\n",
       "      <th>fiscal_month_of_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_year_month</th>\n",
       "      <th>fiscal_year_qtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000/01/01</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>2000/01/02</td>\n",
       "      <td>01/02/2000</td>\n",
       "      <td>02/01/2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date    date_name date_name_us date_name_eu  day_of_week  \\\n",
       "0  20000101  2000-01-01  2000/01/01   01/01/2000   01/01/2000             7   \n",
       "1  20000102  2000-01-02  2000/01/02   01/02/2000   02/01/2000             1   \n",
       "\n",
       "  day_name_of_week  day_of_month  day_of_year weekday_weekend  ...  \\\n",
       "0       Saturday               1            1      Weekend     ...   \n",
       "1       Sunday                 2            2      Weekend     ...   \n",
       "\n",
       "   is_last_day_of_month calendar_quarter  calendar_year calendar_year_month  \\\n",
       "0                     N                1           2000          2000-01      \n",
       "1                     N                1           2000          2000-01      \n",
       "\n",
       "   calendar_year_qtr  fiscal_month_of_year fiscal_quarter fiscal_year  \\\n",
       "0         2000Q1                         7              3        2000   \n",
       "1         2000Q1                         7              3        2000   \n",
       "\n",
       "   fiscal_year_month  fiscal_year_qtr  \n",
       "0         2000-07          2000Q3      \n",
       "1         2000-07          2000Q3      \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2543a-d492-4af3-80d6-8c0d698439c7",
   "metadata": {},
   "source": [
    "#### 3.2. Populate the <span style=\"color:darkred\">Product Dimension</span>\n",
    "##### 3.2.1. Fetch data from the <span style=\"color:darkred\">Employees</span> table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd74e2f1-f996-4d8c-a3ea-f1e72bade787",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_employees = f\"SELECT * FROM {mysql_args['db_name']}.employee\"\n",
    "df_dim_employees = get_mysql_dataframe(spark, sql_dim_employees, **mysql_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828c81b-753a-485a-9942-62d1ae403662",
   "metadata": {},
   "source": [
    "##### 3.2.2. Perform any Necessary Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abf90ea8-49c5-4c2a-b8b6-f8954db33ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_key</th>\n",
       "      <th>employee_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>title</th>\n",
       "      <th>reports_to</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>hire_date</th>\n",
       "      <th>phone</th>\n",
       "      <th>fax</th>\n",
       "      <th>email</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>postal_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>Adams</td>\n",
       "      <td>General Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1962-02-18</td>\n",
       "      <td>2002-08-14</td>\n",
       "      <td>+1 (780) 428-9482</td>\n",
       "      <td>+1 (780) 428-3457</td>\n",
       "      <td>andrew@chinookcorp.com</td>\n",
       "      <td>11120 Jasper Ave NW</td>\n",
       "      <td>Edmonton</td>\n",
       "      <td>AB</td>\n",
       "      <td>Canada</td>\n",
       "      <td>T5K 2N1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>Edwards</td>\n",
       "      <td>Sales Manager</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1958-12-08</td>\n",
       "      <td>2002-05-01</td>\n",
       "      <td>+1 (403) 262-3443</td>\n",
       "      <td>+1 (403) 262-3322</td>\n",
       "      <td>nancy@chinookcorp.com</td>\n",
       "      <td>825 8 Ave SW</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>AB</td>\n",
       "      <td>Canada</td>\n",
       "      <td>T2P 2T3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_key  employee_id first_name last_name            title  \\\n",
       "0             1            1     Andrew     Adams  General Manager   \n",
       "1             2            2      Nancy   Edwards    Sales Manager   \n",
       "\n",
       "   reports_to  birth_date   hire_date              phone                fax  \\\n",
       "0         NaN  1962-02-18  2002-08-14  +1 (780) 428-9482  +1 (780) 428-3457   \n",
       "1         1.0  1958-12-08  2002-05-01  +1 (403) 262-3443  +1 (403) 262-3322   \n",
       "\n",
       "                    email              address      city state country  \\\n",
       "0  andrew@chinookcorp.com  11120 Jasper Ave NW  Edmonton    AB  Canada   \n",
       "1   nancy@chinookcorp.com         825 8 Ave SW   Calgary    AB  Canada   \n",
       "\n",
       "  postal_code  \n",
       "0     T5K 2N1  \n",
       "1     T2P 2T3  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the columns\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "df_dim_employees = (\n",
    "    df_dim_employees\n",
    "    .withColumnRenamed(\"EmployeeId\", \"employee_id\")\n",
    "    .withColumnRenamed(\"LastName\", \"last_name\")\n",
    "    .withColumnRenamed(\"FirstName\", \"first_name\")\n",
    "    .withColumnRenamed(\"Title\", \"title\")\n",
    "    .withColumnRenamed(\"ReportsTo\", \"reports_to\")\n",
    "    .withColumnRenamed(\"BirthDate\", \"birth_date\")\n",
    "    .withColumnRenamed(\"HireDate\", \"hire_date\")\n",
    "    .withColumnRenamed(\"Address\", \"address\")\n",
    "    .withColumnRenamed(\"City\", \"city\")\n",
    "    .withColumnRenamed(\"State\", \"state\")\n",
    "    .withColumnRenamed(\"Country\", \"country\")\n",
    "    .withColumnRenamed(\"PostalCode\", \"postal_code\")\n",
    "    .withColumnRenamed(\"Phone\", \"phone\")\n",
    "    .withColumnRenamed(\"Fax\", \"fax\")\n",
    "    .withColumnRenamed(\"Email\", \"email\")\n",
    ")\n",
    "\n",
    "df_dim_employees = (\n",
    "    df_dim_employees\n",
    "    .withColumn(\"birth_date\", date_format(col(\"birth_date\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"hire_date\", date_format(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using the SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "df_dim_employees.createOrReplaceTempView(\"employees\")\n",
    "sql_employees = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY employee_id) AS employee_key\n",
    "    FROM employees;\n",
    "\"\"\"\n",
    "df_dim_employees = spark.sql(sql_employees)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "ordered_columns = [\n",
    "    'employee_key', 'employee_id', 'first_name', 'last_name'\n",
    "    , 'title', 'reports_to', 'birth_date', 'hire_date'\n",
    "    ,'phone','fax', 'email', 'address'\n",
    "    , 'city','state', 'country', 'postal_code'\n",
    "]\n",
    "\n",
    "df_dim_employees = df_dim_employees[ordered_columns]\n",
    "df_dim_employees.toPandas().head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365dee0a-24ab-467d-bef3-112f6dfc0623",
   "metadata": {},
   "source": [
    "##### 3.2.3. Save as the <span style=\"color:darkred\">dim_employees</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97bd0763-6c8a-48c6-94c0-bdb0605c8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_employees.write.saveAsTable(f\"{dest_database}.dim_employees\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526dedab-0bf0-4fcd-917c-d0a6cb45e1ac",
   "metadata": {},
   "source": [
    "##### 3.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9eba2719-3678-4fc3-9267-281ad38b3fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------+\n",
      "|            col_name|    data_type|comment|\n",
      "+--------------------+-------------+-------+\n",
      "|        employee_key|          int|   NULL|\n",
      "|         employee_id|          int|   NULL|\n",
      "|          first_name|  varchar(20)|   NULL|\n",
      "|           last_name|  varchar(20)|   NULL|\n",
      "|               title|  varchar(30)|   NULL|\n",
      "|          reports_to|          int|   NULL|\n",
      "|          birth_date|       string|   NULL|\n",
      "|           hire_date|       string|   NULL|\n",
      "|               phone|  varchar(24)|   NULL|\n",
      "|                 fax|  varchar(24)|   NULL|\n",
      "|               email|  varchar(60)|   NULL|\n",
      "|             address|  varchar(70)|   NULL|\n",
      "|                city|  varchar(40)|   NULL|\n",
      "|               state|  varchar(40)|   NULL|\n",
      "|             country|  varchar(40)|   NULL|\n",
      "|         postal_code|  varchar(10)|   NULL|\n",
      "|                    |             |       |\n",
      "|# Detailed Table ...|             |       |\n",
      "|             Catalog|spark_catalog|       |\n",
      "|            Database|  chinook_dlh|       |\n",
      "+--------------------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_key</th>\n",
       "      <th>employee_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>title</th>\n",
       "      <th>reports_to</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>hire_date</th>\n",
       "      <th>phone</th>\n",
       "      <th>fax</th>\n",
       "      <th>email</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>postal_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>Adams</td>\n",
       "      <td>General Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1962-02-18</td>\n",
       "      <td>2002-08-14</td>\n",
       "      <td>+1 (780) 428-9482</td>\n",
       "      <td>+1 (780) 428-3457</td>\n",
       "      <td>andrew@chinookcorp.com</td>\n",
       "      <td>11120 Jasper Ave NW</td>\n",
       "      <td>Edmonton</td>\n",
       "      <td>AB</td>\n",
       "      <td>Canada</td>\n",
       "      <td>T5K 2N1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>Edwards</td>\n",
       "      <td>Sales Manager</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1958-12-08</td>\n",
       "      <td>2002-05-01</td>\n",
       "      <td>+1 (403) 262-3443</td>\n",
       "      <td>+1 (403) 262-3322</td>\n",
       "      <td>nancy@chinookcorp.com</td>\n",
       "      <td>825 8 Ave SW</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>AB</td>\n",
       "      <td>Canada</td>\n",
       "      <td>T2P 2T3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_key  employee_id first_name last_name            title  \\\n",
       "0             1            1     Andrew     Adams  General Manager   \n",
       "1             2            2      Nancy   Edwards    Sales Manager   \n",
       "\n",
       "   reports_to  birth_date   hire_date              phone                fax  \\\n",
       "0         NaN  1962-02-18  2002-08-14  +1 (780) 428-9482  +1 (780) 428-3457   \n",
       "1         1.0  1958-12-08  2002-05-01  +1 (403) 262-3443  +1 (403) 262-3322   \n",
       "\n",
       "                    email              address      city state country  \\\n",
       "0  andrew@chinookcorp.com  11120 Jasper Ave NW  Edmonton    AB  Canada   \n",
       "1   nancy@chinookcorp.com         825 8 Ave SW   Calgary    AB  Canada   \n",
       "\n",
       "  postal_code  \n",
       "0     T5K 2N1  \n",
       "1     T2P 2T3  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_employees;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_employees LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893bd291-1833-4271-a0ae-bc72a9a9245f",
   "metadata": {},
   "source": [
    "### 4.0. Verify Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6bc3a90-1b8e-4068-8fba-5ab571844ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chinook_dlh</td>\n",
       "      <td>dim_customers</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chinook_dlh</td>\n",
       "      <td>dim_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chinook_dlh</td>\n",
       "      <td>dim_employees</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chinook_dlh</td>\n",
       "      <td>dim_track</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>customers</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>employees</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>tracks</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     namespace      tableName  isTemporary\n",
       "0  chinook_dlh  dim_customers        False\n",
       "1  chinook_dlh       dim_date        False\n",
       "2  chinook_dlh  dim_employees        False\n",
       "3  chinook_dlh      dim_track        False\n",
       "4                   customers         True\n",
       "5                   employees         True\n",
       "6                      tracks         True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd07030-1ec3-4fec-be31-be22c4aa41e3",
   "metadata": {},
   "source": [
    "## Section III: Integrate Reference Data with Real-Time Data\n",
    "### 5.0. Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Invoice</span> Fact Data  \n",
    "#### 5.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ab486f1-247e-4ad3-8c37-adf0d25f7016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>invoices_batch_1.json</td>\n",
       "      <td>260218</td>\n",
       "      <td>2025-12-16 04:48:57.457730055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>invoices_batch_2.json</td>\n",
       "      <td>262881</td>\n",
       "      <td>2025-12-16 04:48:57.463805676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>invoices_batch_3.json</td>\n",
       "      <td>263235</td>\n",
       "      <td>2025-12-16 04:48:57.471902847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name    size             modification_time\n",
       "0  invoices_batch_1.json  260218 2025-12-16 04:48:57.457730055\n",
       "1  invoices_batch_2.json  262881 2025-12-16 04:48:57.463805676\n",
       "2  invoices_batch_3.json  263235 2025-12-16 04:48:57.471902847"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(invoices_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d52fd-0eb9-43f0-bb7b-5ae0a96010b0",
   "metadata": {},
   "source": [
    "#### 5.2. Create the Bronze Layer: Stage <span style=\"color:darkred\">Invoice Fact table</span> Data\n",
    "##### 5.2.1. Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca2db803-7881-41b0-900a-4ea0a037ee94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_invoice_sales_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"schemaLocation\", invoice_sales_output_bronze) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(invoices_stream_dir)\n",
    ")\n",
    "\n",
    "df_invoice_sales_bronze.isStreaming\n",
    "\n",
    "# should return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843d3ea-e8c3-4c05-890b-6bd6eaf678c6",
   "metadata": {},
   "source": [
    "##### 5.2.2. Write the Streaming Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9534e11-f4d9-4690-91e0-c94728c4f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_sales_checkpoint_bronze = os.path.join(invoice_sales_output_bronze, '_checkpoint')\n",
    "\n",
    "invoice_sales_bronze_query = (\n",
    "    df_invoice_sales_bronze\n",
    "    # Add Current Timestamp and Input Filename columns for Traceability\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    \n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"invoice_sales_bronze\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", invoice_sales_checkpoint_bronze) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(invoice_sales_output_bronze)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b430c01-70b2-42c8-a255-ac2022dff298",
   "metadata": {},
   "source": [
    "##### 5.2.3. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27ac62f9-545a-4ab8-ae99-384baa096f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 945711db-5d3f-4172-aaab-e71f5946dd3e\n",
      "Query Name: invoice_sales_bronze\n",
      "Query Status: {'message': 'Getting offsets from FileStreamSource[file:/C:/Users/emili/ds2002/ds-2002-capstone/lab_data/chinook/streaming/invoices]', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {invoice_sales_bronze_query.id}\")\n",
    "print(f\"Query Name: {invoice_sales_bronze_query.name}\")\n",
    "print(f\"Query Status: {invoice_sales_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2856a331-e654-4245-831a-09764720aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_sales_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18838a-61fa-4d06-9ff8-c1e69dbacb01",
   "metadata": {},
   "source": [
    "#### 5.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 5.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0445695-993e-48f3-97bd-937ad2a3971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_invoice_date = df_dim_date.select(col(\"date_key\").alias(\"invoice_date_key\"), col(\"full_date\").alias(\"invoice_full_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5b179-d30a-47e7-9793-339827c7e63b",
   "metadata": {},
   "source": [
    "##### 5.3.2. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f38d3a1f-7387-49cb-b8aa-61abd2088636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_sales_silver = spark.readStream.format(\"parquet\").load(invoice_sales_output_bronze) \\\n",
    "    .withColumnRenamed(\"InvoiceId\", \"invoice_id\") \\\n",
    "    .withColumnRenamed(\"CustomerId\", \"customer_id\") \\\n",
    "    .withColumnRenamed(\"BillingAddress\", \"billing_address\") \\\n",
    "    .withColumnRenamed(\"BillingCity\", \"billing_city\") \\\n",
    "    .withColumnRenamed(\"BillingState\", \"billing_state\") \\\n",
    "    .withColumnRenamed(\"BillingCountry\", \"billing_country\") \\\n",
    "    .withColumnRenamed(\"BillingPostalCode\", \"billing_postal_code\") \\\n",
    "    .withColumnRenamed(\"Total\", \"invoice_total\") \\\n",
    "    .withColumnRenamed(\"InvoiceLineId\", \"invoice_line_id\") \\\n",
    "    .withColumnRenamed(\"TrackId\", \"track_id\") \\\n",
    "    .withColumnRenamed(\"UnitPrice\", \"invoice_unit_price\") \\\n",
    "    .withColumnRenamed(\"Quantity\", \"quantity\") \\\n",
    "    .withColumn(\"invoice_date\", date_format((col(\"InvoiceDate\") / 1000).cast(TimestampType()), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"invoice_date_for_join\", to_date((col(\"InvoiceDate\") / 1000).cast(TimestampType()))) \\\n",
    "    .join(df_dim_customers, \"customer_id\", \"inner\") \\\n",
    "    .join(df_dim_track, \"track_id\", \"inner\") \\\n",
    "    .join(df_dim_invoice_date, \n",
    "          df_dim_invoice_date.invoice_full_date.cast(DateType()) == col(\"invoice_date_for_join\").cast(DateType()), \n",
    "          \"inner\") \\\n",
    "    .withColumn(\"line_total\", col(\"invoice_unit_price\") * col(\"quantity\")) \\\n",
    "    .select(\n",
    "        col(\"invoice_id\").cast(LongType()),\n",
    "        col(\"invoice_line_id\").cast(LongType()),\n",
    "        col(\"invoice_date\"),\n",
    "        df_dim_invoice_date.invoice_date_key.cast(LongType()),\n",
    "        df_dim_customers.customer_key.cast(LongType()),\n",
    "        df_dim_track.track_key.cast(LongType()),\n",
    "        col(\"quantity\").cast(IntegerType()),\n",
    "        col(\"invoice_unit_price\").alias(\"unit_price\").cast(DoubleType()),\n",
    "        col(\"line_total\").cast(DoubleType()),\n",
    "        col(\"invoice_total\").cast(DoubleType()),\n",
    "        col(\"receipt_time\"),\n",
    "        col(\"source_file\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fbe4b25d-3415-4790-81d6-d5d6cac10688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_invoice_sales_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17e504d9-feca-4d31-9a64-4b14d7a7f7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- invoice_id: long (nullable = true)\n",
      " |-- invoice_line_id: long (nullable = true)\n",
      " |-- invoice_date: string (nullable = true)\n",
      " |-- invoice_date_key: long (nullable = true)\n",
      " |-- customer_key: long (nullable = false)\n",
      " |-- track_key: long (nullable = false)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- line_total: double (nullable = true)\n",
      " |-- invoice_total: double (nullable = true)\n",
      " |-- receipt_time: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_invoice_sales_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688b71c-9be9-406a-98cb-f3364decd311",
   "metadata": {},
   "source": [
    "##### 5.3.3. Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "159a4e83-3d3c-4196-8af9-f6b1df87c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_sales_checkpoint_silver = os.path.join(invoice_sales_output_silver, '_checkpoint')\n",
    "\n",
    "invoice_sales_silver_query = (\n",
    "    df_invoice_sales_silver.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"invoice_sales_silver\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", invoice_sales_checkpoint_silver) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(invoice_sales_output_silver)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b179f-7d77-4386-bca4-298014fa7991",
   "metadata": {},
   "source": [
    "##### 5.3.4. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cbc66582-dea3-4fa4-800a-eb287cd41d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 47cbac32-0c2f-4559-aa70-01c5aac80bd1\n",
      "Query Name: invoice_sales_silver\n",
      "Query Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {invoice_sales_silver_query.id}\")\n",
    "print(f\"Query Name: {invoice_sales_silver_query.name}\")\n",
    "print(f\"Query Status: {invoice_sales_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e28de8f5-88af-424c-8694-cd53ec9769e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_sales_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0ffbf-b112-46b6-9ee4-acf79f99e14f",
   "metadata": {},
   "source": [
    "#### 5.4. Create Gold Layer: Perform Aggregations\n",
    "##### 5.4.1. Define a Query to Create a Business Report\n",
    "Create a new Gold table using the PySpark API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f8968e5f-3006-499f-858b-447e374b94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_sales_by_genre_gold = spark.readStream.format(\"parquet\").load(invoice_sales_output_silver) \\\n",
    ".join(df_dim_track, \"track_key\") \\\n",
    ".join(df_dim_date, df_dim_date.date_key.cast(IntegerType()) == col(\"invoice_date_key\").cast(IntegerType())) \\\n",
    ".groupBy(\"month_of_year\", \"genre_name\", \"month_name\") \\\n",
    ".agg(count(\"invoice_line_id\").alias(\"tracks_sold\")) \\\n",
    ".orderBy(asc(\"month_of_year\"), desc(\"tracks_sold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75019339-5278-4fb9-959c-c194970dcc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month_of_year: byte (nullable = true)\n",
      " |-- genre_name: string (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- tracks_sold: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_invoice_sales_by_genre_gold.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed662d-ba79-40f9-990f-99e496b82d7b",
   "metadata": {},
   "source": [
    "##### 5.4.2. Write the Streaming data to a Parquet File in \"Complete\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "24f54902-5ee4-4820-a277-a5ec0da5966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_sales_gold_query = (\n",
    "    df_invoice_sales_by_genre_gold.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"fact_invoice_sales_by_genre\") \\\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "38057616-8f12-4c5f-a445-fc7b7ed7f989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 1 batchs\n"
     ]
    }
   ],
   "source": [
    "wait_until_stream_is_ready(invoice_sales_gold_query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca737a72-2690-4ddb-b67d-4516fdaf11be",
   "metadata": {},
   "source": [
    "##### 5.4.3. Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6229872c-100e-49d8-b2ec-3a0cb9361b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month_of_year: byte (nullable = true)\n",
      " |-- genre_name: string (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- tracks_sold: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_invoice_sales_by_genre = spark.sql(\"SELECT * FROM fact_invoice_sales_by_genre\")\n",
    "df_fact_invoice_sales_by_genre.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012d326-f065-47a8-af9e-ca2bf6e803db",
   "metadata": {},
   "source": [
    "##### 5.4.4 Create the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cc4b8a7c-2f9f-46cd-aebc-6b8dc97a282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_invoice_sales_by_genre_gold_final = df_fact_invoice_sales_by_genre \\\n",
    ".select(col(\"month_name\").alias(\"Month\"), \\\n",
    "        col(\"genre_name\").alias(\"Genre\"), \\\n",
    "        col(\"tracks_sold\").alias(\"Tracks Sold\"), \\\n",
    "        col(\"month_of_year\")) \\\n",
    ".orderBy(asc(\"month_of_year\"), desc(\"Tracks Sold\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f623d98-2f36-4f81-9ceb-defc053078da",
   "metadata": {},
   "source": [
    "##### 5.4.5. Load the Final Results into a New Table and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1c41c43-c8ca-4203-affc-0cabff6ed661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Tracks Sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>July</td>\n",
       "      <td>Blues</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>July</td>\n",
       "      <td>Rock And Roll</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>August</td>\n",
       "      <td>Latin</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>August</td>\n",
       "      <td>Blues</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>August</td>\n",
       "      <td>Alternative &amp; Punk</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>May</td>\n",
       "      <td>Metal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>June</td>\n",
       "      <td>Rock</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>June</td>\n",
       "      <td>Latin</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>June</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>June</td>\n",
       "      <td>Alternative &amp; Punk</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Month               Genre  Tracks Sold\n",
       "0   July                     Blues            1\n",
       "1   July             Rock And Roll            1\n",
       "2   August                   Latin           21\n",
       "3   August                   Blues           13\n",
       "4   August      Alternative & Punk            8\n",
       "..         ...                 ...          ...\n",
       "72  May                      Metal            1\n",
       "73  June                      Rock           13\n",
       "74  June                     Latin           10\n",
       "75  June                      Jazz            7\n",
       "76  June        Alternative & Punk            6\n",
       "\n",
       "[77 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_invoice_sales_by_genre_gold_final.write.saveAsTable(f\"{dest_database}.fact_invoice_sales_by_genre\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT Month, Genre, `Tracks Sold` FROM {dest_database}.fact_invoice_sales_by_genre\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pysparkenv)",
   "language": "python",
   "name": "pysparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
